# Architectures of Big Data Systems project

Author: Mihajlo Perendija


# Goal

Create a system for real-time notifying about reported crimes in the city of Chicago and analysis of historical data of reported crimes combined with other relevant data.

# Data

Data used in this project consists of:

- [Crimes - 2001 to Present](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2) - Data of reported crimes in Chicago from 2001 to Present in form of a csv file. Attributes of a single report can be seen at the given link. Not available in the repository because of its size (~1.7GB).
- [Police Stations](https://data.cityofchicago.org/Public-Safety/Police-Stations/z8bn-74gv) - Consists of data about police stations - located at *consumers/map/Police_Stations.csv*
- [Boundaries - Police Districts](https://data.cityofchicago.org/Public-Safety/Boundaries-Police-Districts-current-/fthy-xz3r) - Exported as geojson, represents police district boundaries - located at *consumers/map/Boundaries - Police Districts (current).geojson*
- Census Data By Community Area - Found in pdf form, transformed to csv, consists of data about population in all community areas in Chicago - located at *data/Census_Data_By_Community_Area.csv*
- Not used in project but available in repositry: Police Beats, Census Data - Socioeconomics data

Preparation of data:
After downloading *"Crimes - 2001 to Present"* dataset it should be split in two files: 
- data_batch.csv - Consisting of the majority of the data from the dataset as it is used in batch processing of historical data. This file should be placed in *data* folder.
- data_realtime.csv - Consisting of the last, small part of the dataset as it will be used to generate "new" data for real-time processing. This file should be placed in *producer* folder. Also, it should be cleaned of empty values for locations.

# Architecture

![Architecture](https://github.com/mihajlo-perendija/ASVSP/blob/master/system_architecture.png)

On the image above we can see two main parts of the system. 
- Real-time processing - Consists of data producer, Kafka topics and stream processing, and a map to see incoming data. <br> Goal of this part of the system is to inform users about new reported crimes in real-time. <br> Producer sends row by row from data_realtime.csv based on its internal clock that is sped up 60 times. <br> Those events are kept in Kafka topics and consumed by application located in *consumer/map* folder.
- Batch processing - Analysis of historical data. Data located in HDFS is processed with Spark and results are saved in MongoDB so they can be visualized by the analytics dashboard app. <br> New data generated by real-time producer can be ingested in HDFS for future analytic queries (*Not implemented yet*). <br> Spark jobs that are implemented are located in *spark-jobs* folder.

# Prerequisites and startup of the system

To run this system on your machine you will need to have Docker and Python with some libraries. <br>
The main parts of the system are dockerized and others like real-time map and Kafka streaming are currently run in a local environment for better control. These can be dockerized in the same way as the real-time producer is, which you can see in *producer* folder, and added to *docker-compose.yaml* file.


These are the steps to run this project:

1. Download *"Crimes - 2001 to Present"* dataset. Split it into batch and real-time files as explained above and place them in the right directories.
2. Go to the *docker-airflow* directory and run *build* script. This builds the image for the container running the Apache Airflow.
3. Start docker containers with docker-compose command - docker-compose up 

To be able to run analytics jobs in spark you will need to place relevant data in HDFS, to do this follow these commands:
1. Position yourself in the root folder, open the terminal of your choice and type the next commands.
2. docker cp /data hadoop-namenode:/home
3. docker exec -it hadoop-namenode bash
4. cd home
5. hdfs dfs -mkdir /data
6. hdfs dfs -put data/ data/

To startup real-time processing do the following:
1. Install Python libraries found in *requirements.txt* files located in *kafka-jobs/streaming* and *consumers/map* directories.
2. Go to *kafka-jobs/streaming* directory and in terminal type: python main.py worker
3. Go to *consumers/map* directory and in terminal type: bokeh serve --show map.py
4. New tab in your browser should open and you can see the real-time interactive map.
<br> *Note: As retention periods of Kafka topics have not been implemented as planned, the consumer application should be stopped after the first startup and started again, this will be fixed in the next version.*

To run batch jobs and visualize results do the following:
1. Go to the localhost:8282 to open Apache Airflow UI. 
2. Run the analytics_dag and monitor the progress of jobs in the DAG. *Note: Keep in mind that this can take a long time depending on your computer resources.*
3. After all of the jobs, or some of the analytic ones, are completed, go to the localhost:3001 to open the Metabase app to see and visualize the results stored in MongoDB.
4. You can also view the logs of spark jobs in airflow if you don't wish to set up the Metabase app or use Mongo Express (localhost:8085) to see the data in MongoDB.

